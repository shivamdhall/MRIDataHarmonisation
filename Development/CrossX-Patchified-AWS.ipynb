{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - CrossX - Patch - AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use both scans of subjects 1-8 from the PETMR and TRIO dataset for training.\n",
    "\n",
    "We used the scan of subjects 9-10 also from the PETMR and TRIO dataset for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from dipy.io import read_bvals_bvecs\n",
    "from dipy.core.gradients import gradient_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function simply uploads the testing and training scans into lists of numpy arrays\n",
    "# The data is not yet sliced or patched at this stage\n",
    "\n",
    "# Specify in a list what scans to use for training and what scans to use for testing\n",
    "\n",
    "# This function only takes the first and last b=0 volumes\n",
    "\n",
    "def get_data(petmr_path, trio_path, scans_dict):\n",
    "    \n",
    "    train_data_inp = []\n",
    "    test_data_inp = []\n",
    "    train_data_out = []\n",
    "    test_data_out = []\n",
    "    paths = [petmr_path, trio_path]\n",
    "    \n",
    "    for data_path in paths:\n",
    "        if(data_path == petmr_path):\n",
    "            print \"Uploading Inputs:\"\n",
    "            training_data_store = train_data_inp\n",
    "            testing_data_store = test_data_inp\n",
    "        else:\n",
    "            print \"Uploading Outputs\"\n",
    "            training_data_store = train_data_out\n",
    "            testing_data_store = test_data_out\n",
    "        os.chdir(data_path)\n",
    "        for key, subjs in scans_dict.iteritems():\n",
    "            for subj_scan in subjs:\n",
    "                scan_image = nib.load(str(data_path) + \"/Subj\" + subj_scan + \"/Brain_Thresholded.nii.gz\")\n",
    "                scan_data = scan_image.get_data()\n",
    "                #all scans have the same affine mat because registration has already been performed\n",
    "                #we only need it for saving the predictions as a NIfTI file\n",
    "                affine_mat = scan_image.affine\n",
    "                #get b=0 volumes only\n",
    "                bvals_scan, bvecs_scan = read_bvals_bvecs(str(data_path) + \"/Subj\" + subj_scan + \"/NODDI.bval\",\\\n",
    "                                                          str(data_path) + \"/Subj\" + subj_scan + \"/NODDI.bvec\")\n",
    "                #set a threshold value for b=0 values (due to TRIO dataset)\n",
    "                gtab_scan = gradient_table(bvals_scan, bvecs_scan, b0_threshold=5)\n",
    "                s0s_scan = scan_data[:, :, :, gtab_scan.b0s_mask]\n",
    "            \n",
    "                if(key == \"training\"):\n",
    "                    print (\"Training: Subj%s\" % subj_scan)\n",
    "                    #append this data to the list containing the training data\n",
    "                    training_data_store.append(s0s_scan[:,:,:,[0,-1]])\n",
    "                else:\n",
    "                    print (\"Testing: Subj%s\" % subj_scan)\n",
    "                    testing_data_store.append(s0s_scan[:,:,:,[0,-1]])\n",
    "    return (train_data_inp, train_data_out, test_data_inp, test_data_out, affine_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function extracts patches from a scan and appends them to a list.\n",
    "#This is done for all volumes of a scan (for both the inputs and targets)\n",
    "\n",
    "def patchify(data_scans_inp, data_scans_out, patch_size):\n",
    "    \n",
    "    input_patches_store = []\n",
    "    target_patches_store = []\n",
    "    \n",
    "    for scan_no in range(0, len(data_scans_inp)):\n",
    "        input_scan = data_scans_inp[scan_no]\n",
    "        target_scan = data_scans_out[scan_no]\n",
    "        \n",
    "        (x,y,z,v) = input_scan.shape\n",
    "        if((patch_size-1) % 2 != 0):\n",
    "            print \"The patch size is not compatible\"\n",
    "            return\n",
    "        padding = ((patch_size-1)/2)\n",
    "        #pad the input scan\n",
    "        full_padding = ((padding, padding), (padding, padding), (padding,padding), (0,0))\n",
    "        padded_scan = np.pad(input_scan, full_padding, mode='constant', constant_values=0)\n",
    "\n",
    "        #extract patches from the input scan\n",
    "\n",
    "        #iterate through each volume to extract the patches\n",
    "        for volume in range(0, v):\n",
    "            for pos_x in range(0, x):\n",
    "                for pos_y in range(0,y):\n",
    "                    for pos_z in range(0, z):\n",
    "                        input_patch = padded_scan[pos_x:pos_x+patch_size, pos_y:pos_y+patch_size, pos_z:pos_z+patch_size, volume]\n",
    "                        target_voxel = target_scan[pos_x,pos_y,pos_z,volume]\n",
    "\n",
    "                        #store the patch and the target\n",
    "                        input_patches_store.append(input_patch)\n",
    "                        target_patches_store.append(target_voxel)\n",
    "                    \n",
    "    return (input_patches_store, target_patches_store)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function extracts patches from a scan and appends them to a list.\n",
    "#This is done for all volumes of a scan (for both the inputs and targets)\n",
    "#This version of the functions only considers voxels wholly contained within the brain\n",
    "\n",
    "def patchify_brain_only(data_scans_inp, data_scans_out, patch_size):\n",
    "    \n",
    "    input_patches_store = []\n",
    "    target_patches_store = []\n",
    "    \n",
    "    for scan_no in range(0, len(data_scans_inp)):\n",
    "        input_scan = data_scans_inp[scan_no]\n",
    "        target_scan = data_scans_out[scan_no]\n",
    "        \n",
    "        (x,y,z,v) = input_scan.shape\n",
    "        if((patch_size-1) % 2 != 0):\n",
    "            print \"The patch size is not compatible\"\n",
    "            return\n",
    "        padding = ((patch_size-1)/2)\n",
    "        #pad the input scan\n",
    "        full_padding = ((padding, padding), (padding, padding), (padding,padding), (0,0))\n",
    "        padded_scan = np.pad(input_scan, full_padding, mode='constant', constant_values=0)\n",
    "\n",
    "        #extract patches from the input scan\n",
    "        #use unpadded scan (original input scan) to identify non-backround voxels\n",
    "\n",
    "        #iterate through each volume to extract the patches\n",
    "        for volume in range(0, v):\n",
    "            for pos_x in range(0, x):\n",
    "                for pos_y in range(0,y):\n",
    "                    for pos_z in range(0, z):\n",
    "                        # Exclude all background voxels\n",
    "                        if(input_scan[pos_x,pos_y,pos_z,volume] == 0):\n",
    "                            continue\n",
    "                        input_patch = padded_scan[pos_x:pos_x+patch_size, pos_y:pos_y+patch_size, pos_z:pos_z+patch_size, volume]\n",
    "                        target_voxel = target_scan[pos_x,pos_y,pos_z,volume]\n",
    "                       \n",
    "                        #store the patch and the target\n",
    "                        input_patches_store.append(input_patch)\n",
    "                        target_patches_store.append(target_voxel)\n",
    "                    \n",
    "    return (input_patches_store, target_patches_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(dataset, mean=None, std=None):\n",
    "    data_array = np.array(dataset)\n",
    "    if mean==None and std==None:\n",
    "        #This is the training data\n",
    "        mean = np.mean(data_array)\n",
    "        std = np.std(data_array)\n",
    "    #normalise the data\n",
    "    data_array = (data_array - mean)/std\n",
    "    return (data_array, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_pred(inputs, predictions, labels, sliceNo):\n",
    "    plt.figure\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1, 3, 1).set_axis_off()\n",
    "    plt.imshow(inputs[:,:,sliceNo,0].T, cmap='gray', origin='lower')\n",
    "    plt.title(\"Input\")\n",
    "    plt.subplot(1, 3, 2).set_axis_off()\n",
    "    plt.imshow(predictions[:,:,sliceNo,0].T, cmap='gray', origin='lower')\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.subplot(1, 3, 3).set_axis_off()\n",
    "    plt.imshow(labels[:,:,sliceNo,0].T, cmap='gray', origin='lower')\n",
    "    plt.title(\"Target\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------\n",
    "---\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Inputs:\n",
      "Training: Subj2Scan1\n",
      "Training: Subj3Scan2\n",
      "Training: Subj4Scan1\n",
      "Training: Subj5Scan2\n",
      "Testing: Subj1Scan2\n",
      "Uploading Outputs\n",
      "Training: Subj2Scan1\n",
      "Training: Subj3Scan2\n",
      "Training: Subj4Scan1\n",
      "Training: Subj5Scan2\n",
      "Testing: Subj1Scan2\n",
      "Number of scans used for training input: 4\n",
      "Number of scans used for training output: 4\n",
      "Number of scans used for testing input: 1\n",
      "Number of scans used for testing output: 1\n"
     ]
    }
   ],
   "source": [
    "#upload the data\n",
    "petmr_data_path = '/home/ubuntu/project/Dataset/PETMR_data'\n",
    "trio_data_path = '/home/ubuntu/project/Dataset/TRIO_data'\n",
    "\n",
    "training_scans = [\"2Scan1\", \"3Scan2\", \"4Scan1\", \"5Scan2\"]\n",
    "\n",
    "\n",
    "testing_scans = [\"1Scan2\"]\n",
    "\n",
    "(training_data_inp, training_data_out, testing_data_inp, testing_data_out, affine_mat) = \\\n",
    "        get_data(petmr_data_path, trio_data_path, {\"training\":training_scans, \"testing\":testing_scans})\n",
    "\n",
    "print (\"Number of scans used for training input: %d\" % len(training_data_inp))\n",
    "print (\"Number of scans used for training output: %d\" % len(training_data_out))\n",
    "print (\"Number of scans used for testing input: %d\" % len(testing_data_inp))\n",
    "print (\"Number of scans used for testing output: %d\" % len(testing_data_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patchify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patchifying training set\n",
      "Patchifying testing set\n"
     ]
    }
   ],
   "source": [
    "print \"Patchifying training set\"\n",
    "(training_input, training_target) = patchify_brain_only(training_data_inp, training_data_out, 9)\n",
    "\n",
    "print \"Patchifying testing set\"\n",
    "(testing_input, testing_target) = patchify(testing_data_inp, testing_data_out, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nember of training examples : 643030\n",
      "Nember of testing examples : 257600\n"
     ]
    }
   ],
   "source": [
    "print (\"Nember of training examples : %d\" % len(training_input))\n",
    "print (\"Nember of testing examples : %d\" % len(testing_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset class for our data\n",
    "\n",
    "class MRIdataset(Dataset):\n",
    "    \"\"\"MRI b=0 dataset for patches.\"\"\"\n",
    "\n",
    "    def __init__(self, input_patches, target_patches, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_patches: Input patches\n",
    "            target_patches: Corresponding target patches of the input patches\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "        \"\"\"\n",
    "        self.input_patches = input_patches\n",
    "        self.target_patches = target_patches\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_patch = np.array(self.input_patches[idx])\n",
    "        target_patch = np.array(self.target_patches[idx])\n",
    "        sample = {'input': input_patch, 'target': target_patch}\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class To_Tensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inp, out = sample['input'], sample['target']\n",
    "        \n",
    "        #first expand dimension because torch expects H x W x D x C\n",
    "        #currently we only have H x W x D\n",
    "        aug_inp = np.expand_dims(inp, 3)\n",
    "        \n",
    "        #The target is a single voxel,\n",
    "        #Conver it to an array\n",
    "        aug_out = np.array([out])\n",
    "\n",
    "        # swap channel axis because\n",
    "        # numpy: H x W x D x C\n",
    "        # torch: C x D x H x W\n",
    "        aug_inp = aug_inp.transpose((3, 2, 0, 1))\n",
    "        \n",
    "        return {'input': torch.Tensor(aug_inp),\n",
    "                'target': torch.Tensor(aug_out)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, training_dataset, trainloader, losses_list, optimizer, criterion, epochs):\n",
    "    net.train()\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): #done in batches\n",
    "            # get the inputs\n",
    "            inputs = data['input']\n",
    "            labels = data['target']\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize/update weights\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0] #loss is a variable tensor of size 1, we index to get the value out\n",
    "            if i % 250 == 249:    # print every 250 mini-batches\n",
    "                print('[%d, %d] Loss = %.5f' % (epoch + 1, i + 1, running_loss/i))\n",
    "        total_loss = running_loss / i\n",
    "        losses_list.append(total_loss)\n",
    "        print('Loss iteration %d = %.5f' % (epoch+1, total_loss ))\n",
    "        '''   \n",
    "        test_error = 0\n",
    "        total = 0\n",
    "        for test_data in testloader: #batch processing\n",
    "            test_inputs = test_data['inp']\n",
    "            test_labels = test_data['out']\n",
    "            total += len(test_labels)\n",
    "\n",
    "            test_outputs = net(Variable(test_inputs))\n",
    "\n",
    "            test_error += (torch.nn.functional.mse_loss(test_outputs.data, test_labels, size_average=False)).data[0]\n",
    "\n",
    "        test_error /= total\n",
    "        print('MSE on test data: %f' % (test_error))\n",
    "        Adam_acc.append(test_error)\n",
    "        '''\n",
    "    print('Finished Training')\n",
    "    return (net, losses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_losses(losses_list):\n",
    "    plt.figure\n",
    "    plt.plot(range(1,len(losses_list)+1), losses_list, 'r-')\n",
    "    plt.xlabel('iteration')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    " def testing_error(net, testloader, loss_type=\"MSE\"):\n",
    "    net.eval()\n",
    "    test_error = 0\n",
    "    total = 0\n",
    "    for test_data in testloader: #batch processing\n",
    "        test_inputs = test_data['input']\n",
    "        test_labels = test_data['target']\n",
    "        total += 1\n",
    "\n",
    "        test_inputs, test_labels = Variable(test_inputs.cuda()), Variable(test_labels.cuda())\n",
    "        \n",
    "        test_predictions = net(test_inputs)\n",
    "        \n",
    "        if(loss_type == \"MSE\"):\n",
    "            #Use MSE loss\n",
    "            test_error += (torch.nn.functional.mse_loss( Variable(test_predictions.data), test_labels)).data[0]\n",
    "        else:\n",
    "            test_error += (torch.nn.functional.l1_loss(Variable(test_predictions.data), test_labels)).data[0]\n",
    "        \n",
    "    test_error /= total\n",
    "    return test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(net, testloader):\n",
    "    net.eval()\n",
    "    for index, test_data in enumerate(testloader):\n",
    "        test_inputs = test_data['input']\n",
    "        test_labels = test_data['target']\n",
    "        if index % 250 == 249:\n",
    "            print index + 1\n",
    "        \n",
    "        test_inputs, test_labels = Variable(test_inputs.cuda()), Variable(test_labels.cuda())\n",
    "\n",
    "        #store the predictions in a numpy array which we can reshape later\n",
    "        test_predictions = net(test_inputs)\n",
    "        if(index == 0):\n",
    "            predictions = test_predictions.data.cpu().numpy() \n",
    "\n",
    "        else:\n",
    "            predictions = np.concatenate((predictions, test_predictions.data.cpu().numpy()), axis=0)\n",
    "            \n",
    "    #convert back to numpy dimensions of (HxWxDxCxNumbExpls)\n",
    "    predictions = predictions.transpose(3,4,2,1,0)\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_background(prediction, input_scan):\n",
    "    background_mask = input_scan <= 0\n",
    "    prediction[background_mask] = 0\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_brain(predictions, dimensions):\n",
    "    \n",
    "    size_x = dimensions[0]\n",
    "    size_y = dimensions[1]\n",
    "    size_z = dimensions[2]\n",
    "    size_v = dimensions[3]\n",
    "    #assume we have given it a single scan to reconstruct\n",
    "    reconstructed = np.reshape(predictions, [size_v, size_x, size_y, size_z], order='C')\n",
    "    reconstructed = reconstructed.transpose(1,2,3,0)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------\n",
    "---\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data using pytorch data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = MRIdataset(training_input, training_target, transform=transforms.Compose([To_Tensor()]))\n",
    "testing_dataset = MRIdataset(testing_input, testing_target, transform=transforms.Compose([To_Tensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(training_dataset, batch_size=160,\n",
    "                        shuffle=True, num_workers=8)\n",
    "testloader = DataLoader(testing_dataset, batch_size=160,\n",
    "                        shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv -> Batch -> Relu -> Conv -> Batch -> Relu -> Conv -> Batch -> Relu -> Conv -> Relu\n",
    "\n",
    "--(WxHx2x1)--\n",
    "\n",
    "conv1 = receptive field -> (3x3x3), zero padding -> 2,  number of filters -> 10\n",
    "\n",
    "--(W+2xH+2x4x10)--\n",
    "\n",
    "--BATCH then RELU--\n",
    "\n",
    "conv2 = receptive field -> (1x1x1), number of filters -> 15\n",
    "\n",
    "--(W+2xH+2x4x15)--\n",
    "\n",
    "--BATCH then RELU--\n",
    "\n",
    "conv3 = receptive field -> (3x3x3), number of filters -> 15\n",
    "\n",
    "--(WxHx2x15)--\n",
    "\n",
    "--BATCH then RELU--\n",
    "\n",
    "conv4 = receptive field -> (1x1x2), number of filters -> 1\n",
    "\n",
    "--(WxHx1x1)--\n",
    "\n",
    "--RELU--\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv3d (1, 50, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (batch1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (drop1): Dropout(p=0.2)\n",
      "  (conv2): Conv3d (50, 100, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  (batch2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (drop2): Dropout(p=0.2)\n",
      "  (conv3): Conv3d (100, 100, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (batch3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (drop3): Dropout(p=0.2)\n",
      "  (conv4): Conv3d (100, 100, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  (batch4): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (drop4): Dropout(p=0.2)\n",
      "  (conv5): Conv3d (100, 100, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "  (batch5): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (drop5): Dropout(p=0.2)\n",
      "  (conv6): Conv3d (100, 50, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  (batch6): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (drop6): Dropout(p=0.2)\n",
      "  (conv7): Conv3d (50, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 50, 3)\n",
    "        self.batch1 = nn.BatchNorm2d(50)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.conv2 = nn.Conv3d(50, 100, 1)\n",
    "        self.batch2 = nn.BatchNorm2d(100)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.conv3 = nn.Conv3d(100, 100, 3)\n",
    "        self.batch3 = nn.BatchNorm2d(100)\n",
    "        self.drop3 = nn.Dropout(p=0.2)\n",
    "        self.conv4 = nn.Conv3d(100, 100, 1)\n",
    "        self.batch4 = nn.BatchNorm2d(100)\n",
    "        self.drop4 = nn.Dropout(p=0.2)\n",
    "        self.conv5 = nn.Conv3d(100, 100, 3)\n",
    "        self.batch5 = nn.BatchNorm2d(100)\n",
    "        self.drop5 = nn.Dropout(p=0.2)\n",
    "        self.conv6 = nn.Conv3d(100, 50, 1)\n",
    "        self.batch6 = nn.BatchNorm2d(50)\n",
    "        self.drop6 = nn.Dropout(p=0.2)\n",
    "        self.conv7 = nn.Conv3d(50, 1, 3)\n",
    "        \n",
    "        \n",
    "\n",
    "    #note this method isn't called explicitly during train, \n",
    "    #rather the instance object is called as pytorch is then \n",
    "    #able to take care of other stuff in the background\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch1(self.conv1(x)))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.batch3(self.conv3(x)))\n",
    "        x = F.relu(self.batch4(self.conv4(x)))\n",
    "        x = F.relu(self.batch5(self.conv5(x)))\n",
    "        x = F.relu(self.batch6(self.conv6(x)))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we use MSE loss\n",
    "criterion = nn.MSELoss() #returns the average over a mini-batch as opposed to the sum\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 250] Loss = 62323.92505\n",
      "[1, 500] Loss = 35604.09164\n",
      "[1, 750] Loss = 25725.13548\n",
      "[1, 1000] Loss = 20636.22685\n",
      "[1, 1250] Loss = 17476.46926\n",
      "[1, 1500] Loss = 15349.94603\n",
      "[1, 1750] Loss = 13788.36984\n",
      "[1, 2000] Loss = 12594.17973\n",
      "[1, 2250] Loss = 11649.55889\n",
      "[1, 2500] Loss = 10870.67740\n",
      "[1, 2750] Loss = 10210.21297\n",
      "[1, 3000] Loss = 9658.54407\n",
      "[1, 3250] Loss = 9181.51415\n",
      "[1, 3500] Loss = 8763.31933\n",
      "[1, 3750] Loss = 8395.01554\n",
      "[1, 4000] Loss = 8059.91023\n",
      "Loss iteration 1 = 8036.14244\n",
      "[2, 250] Loss = 2917.27495\n",
      "[2, 500] Loss = 2828.94457\n",
      "[2, 750] Loss = 2780.65938\n",
      "[2, 1000] Loss = 2740.16140\n",
      "[2, 1250] Loss = 2708.46708\n",
      "[2, 1500] Loss = 2682.65638\n",
      "[2, 1750] Loss = 2654.72296\n",
      "[2, 2000] Loss = 2624.76601\n",
      "[2, 2250] Loss = 2600.18006\n",
      "[2, 2500] Loss = 2574.98097\n",
      "[2, 2750] Loss = 2548.41206\n",
      "[2, 3000] Loss = 2523.71047\n",
      "[2, 3250] Loss = 2497.72697\n",
      "[2, 3500] Loss = 2469.97109\n",
      "[2, 3750] Loss = 2443.44012\n",
      "[2, 4000] Loss = 2416.95034\n",
      "Loss iteration 2 = 2414.79014\n",
      "[3, 250] Loss = 1887.36650\n",
      "[3, 500] Loss = 1850.31586\n",
      "[3, 750] Loss = 1848.48898\n",
      "[3, 1000] Loss = 1841.34596\n",
      "[3, 1250] Loss = 1825.84193\n",
      "[3, 1500] Loss = 1815.76526\n",
      "[3, 1750] Loss = 1807.97720\n",
      "[3, 2000] Loss = 1796.65434\n",
      "[3, 2250] Loss = 1784.02144\n",
      "[3, 2500] Loss = 1774.81414\n",
      "[3, 2750] Loss = 1764.62712\n",
      "[3, 3000] Loss = 1752.63500\n",
      "[3, 3250] Loss = 1742.11984\n",
      "[3, 3500] Loss = 1732.79420\n",
      "[3, 3750] Loss = 1722.49736\n",
      "[3, 4000] Loss = 1713.44077\n",
      "Loss iteration 3 = 1713.23510\n",
      "[4, 250] Loss = 1389.67508\n",
      "[4, 500] Loss = 1403.92518\n",
      "[4, 750] Loss = 1402.59059\n",
      "[4, 1000] Loss = 1404.35406\n",
      "[4, 1250] Loss = 1396.15816\n",
      "[4, 1500] Loss = 1393.08256\n",
      "[4, 1750] Loss = 1392.17152\n",
      "[4, 2000] Loss = 1388.79058\n",
      "[4, 2250] Loss = 1386.30794\n",
      "[4, 2500] Loss = 1381.25785\n",
      "[4, 2750] Loss = 1379.15290\n",
      "[4, 3000] Loss = 1372.93084\n",
      "[4, 3250] Loss = 1366.01343\n",
      "[4, 3500] Loss = 1358.61411\n",
      "[4, 3750] Loss = 1353.85831\n",
      "[4, 4000] Loss = 1349.57731\n",
      "Loss iteration 4 = 1349.29476\n",
      "[5, 250] Loss = 1185.63360\n",
      "[5, 500] Loss = 1189.12564\n",
      "[5, 750] Loss = 1181.85794\n",
      "[5, 1000] Loss = 1162.44285\n",
      "[5, 1250] Loss = 1162.37720\n",
      "[5, 1500] Loss = 1155.80410\n",
      "[5, 1750] Loss = 1155.91853\n",
      "[5, 2000] Loss = 1154.87325\n",
      "[5, 2250] Loss = 1155.43032\n",
      "[5, 2500] Loss = 1158.20555\n",
      "[5, 2750] Loss = 1158.49366\n",
      "[5, 3000] Loss = 1152.94345\n",
      "[5, 3250] Loss = 1150.10987\n",
      "[5, 3500] Loss = 1150.16261\n",
      "[5, 3750] Loss = 1145.18291\n",
      "[5, 4000] Loss = 1141.11999\n",
      "Loss iteration 5 = 1141.04715\n",
      "[6, 250] Loss = 987.36997\n",
      "[6, 500] Loss = 1000.78263\n",
      "[6, 750] Loss = 999.71758\n",
      "[6, 1000] Loss = 1008.05765\n",
      "[6, 1250] Loss = 1003.14742\n",
      "[6, 1500] Loss = 1009.33698\n",
      "[6, 1750] Loss = 1009.74734\n",
      "[6, 2000] Loss = 1012.72589\n",
      "[6, 2250] Loss = 1009.88363\n",
      "[6, 2500] Loss = 1009.32062\n",
      "[6, 2750] Loss = 1007.40668\n",
      "[6, 3000] Loss = 1008.28141\n",
      "[6, 3250] Loss = 1006.30619\n",
      "[6, 3500] Loss = 1006.26805\n",
      "[6, 3750] Loss = 1005.70935\n",
      "[6, 4000] Loss = 1005.36477\n",
      "Loss iteration 6 = 1005.37371\n",
      "[7, 250] Loss = 865.19332\n",
      "[7, 500] Loss = 867.69719\n",
      "[7, 750] Loss = 881.12860\n",
      "[7, 1000] Loss = 882.77659\n",
      "[7, 1250] Loss = 887.60494\n",
      "[7, 1500] Loss = 888.10590\n",
      "[7, 1750] Loss = 888.15947\n",
      "[7, 2000] Loss = 894.03689\n",
      "[7, 2250] Loss = 892.92674\n",
      "[7, 2500] Loss = 893.27350\n",
      "[7, 2750] Loss = 891.15221\n",
      "[7, 3000] Loss = 890.60541\n",
      "[7, 3250] Loss = 892.12697\n",
      "[7, 3500] Loss = 890.55403\n",
      "[7, 3750] Loss = 890.29570\n",
      "[7, 4000] Loss = 890.03120\n",
      "Loss iteration 7 = 890.18675\n",
      "[8, 250] Loss = 762.75894\n",
      "[8, 500] Loss = 772.98587\n",
      "[8, 750] Loss = 781.06686\n",
      "[8, 1000] Loss = 789.22245\n",
      "[8, 1250] Loss = 792.35120\n",
      "[8, 1500] Loss = 796.84038\n",
      "[8, 1750] Loss = 800.26656\n",
      "[8, 2000] Loss = 804.61790\n",
      "[8, 2250] Loss = 801.76537\n",
      "[8, 2500] Loss = 805.93875\n",
      "[8, 2750] Loss = 804.93023\n",
      "[8, 3000] Loss = 804.60708\n",
      "[8, 3250] Loss = 806.37209\n",
      "[8, 3500] Loss = 804.47847\n",
      "[8, 3750] Loss = 802.51506\n",
      "[8, 4000] Loss = 801.60168\n",
      "Loss iteration 8 = 801.58467\n",
      "[9, 250] Loss = 731.87250\n",
      "[9, 500] Loss = 725.17136\n",
      "[9, 750] Loss = 725.81006\n",
      "[9, 1000] Loss = 724.55545\n",
      "[9, 1250] Loss = 730.02782\n",
      "[9, 1500] Loss = 735.19925\n",
      "[9, 1750] Loss = 737.93156\n",
      "[9, 2000] Loss = 738.17505\n",
      "[9, 2250] Loss = 737.66272\n",
      "[9, 2500] Loss = 738.97413\n",
      "[9, 2750] Loss = 741.07622\n",
      "[9, 3000] Loss = 740.93352\n",
      "[9, 3250] Loss = 740.65452\n",
      "[9, 3500] Loss = 740.84255\n",
      "[9, 3750] Loss = 741.31507\n",
      "[9, 4000] Loss = 739.94981\n",
      "Loss iteration 9 = 739.75676\n",
      "[10, 250] Loss = 654.84630\n",
      "[10, 500] Loss = 645.47820\n",
      "[10, 750] Loss = 648.30278\n",
      "[10, 1000] Loss = 654.38194\n",
      "[10, 1250] Loss = 664.56779\n",
      "[10, 1500] Loss = 668.68428\n",
      "[10, 1750] Loss = 670.45627\n",
      "[10, 2000] Loss = 671.54837\n",
      "[10, 2250] Loss = 672.46670\n",
      "[10, 2500] Loss = 672.02962\n",
      "[10, 2750] Loss = 677.54306\n",
      "[10, 3000] Loss = 679.38466\n",
      "[10, 3250] Loss = 681.34614\n",
      "[10, 3500] Loss = 682.80131\n",
      "[10, 3750] Loss = 684.57741\n",
      "[10, 4000] Loss = 683.45144\n",
      "Loss iteration 10 = 683.32985\n",
      "[11, 250] Loss = 622.27846\n",
      "[11, 500] Loss = 625.14863\n",
      "[11, 750] Loss = 625.64003\n",
      "[11, 1000] Loss = 631.96466\n",
      "[11, 1250] Loss = 632.54645\n",
      "[11, 1500] Loss = 634.26825\n",
      "[11, 1750] Loss = 636.29119\n",
      "[11, 2000] Loss = 639.75812\n",
      "[11, 2250] Loss = 640.76105\n",
      "[11, 2500] Loss = 640.35168\n",
      "[11, 2750] Loss = 639.81244\n",
      "[11, 3000] Loss = 641.66049\n",
      "[11, 3250] Loss = 644.25251\n",
      "[11, 3500] Loss = 644.16011\n",
      "[11, 3750] Loss = 645.38193\n",
      "[11, 4000] Loss = 645.65358\n",
      "Loss iteration 11 = 645.94052\n",
      "[12, 250] Loss = 577.82328\n",
      "[12, 500] Loss = 579.18469\n",
      "[12, 750] Loss = 589.39210\n",
      "[12, 1000] Loss = 587.03841\n",
      "[12, 1250] Loss = 589.95522\n",
      "[12, 1500] Loss = 593.64608\n",
      "[12, 1750] Loss = 596.65429\n",
      "[12, 2000] Loss = 598.31006\n",
      "[12, 2250] Loss = 600.37868\n",
      "[12, 2500] Loss = 601.97481\n",
      "[12, 2750] Loss = 601.75206\n",
      "[12, 3000] Loss = 602.15750\n",
      "[12, 3250] Loss = 601.43695\n",
      "[12, 3500] Loss = 602.05475\n",
      "[12, 3750] Loss = 601.21179\n",
      "[12, 4000] Loss = 601.28755\n",
      "Loss iteration 12 = 601.06832\n",
      "[13, 250] Loss = 537.99304\n",
      "[13, 500] Loss = 543.29430\n",
      "[13, 750] Loss = 548.06443\n",
      "[13, 1000] Loss = 556.90192\n",
      "[13, 1250] Loss = 558.10711\n",
      "[13, 1500] Loss = 559.31864\n",
      "[13, 1750] Loss = 559.21234\n",
      "[13, 2000] Loss = 561.24657\n",
      "[13, 2250] Loss = 562.12845\n",
      "[13, 2500] Loss = 564.96770\n",
      "[13, 2750] Loss = 567.84761\n",
      "[13, 3000] Loss = 568.87789\n",
      "[13, 3250] Loss = 568.18308\n",
      "[13, 3500] Loss = 569.84258\n",
      "[13, 3750] Loss = 569.69395\n",
      "[13, 4000] Loss = 569.12262\n",
      "Loss iteration 13 = 569.02322\n",
      "[14, 250] Loss = 514.76045\n",
      "[14, 500] Loss = 512.98723\n",
      "[14, 750] Loss = 507.57489\n",
      "[14, 1000] Loss = 517.94932\n",
      "[14, 1250] Loss = 521.16676\n",
      "[14, 1500] Loss = 522.28947\n",
      "[14, 1750] Loss = 526.02029\n",
      "[14, 2000] Loss = 526.50640\n",
      "[14, 2250] Loss = 526.98663\n",
      "[14, 2500] Loss = 529.87265\n",
      "[14, 2750] Loss = 530.46554\n",
      "[14, 3000] Loss = 530.51216\n",
      "[14, 3250] Loss = 533.80569\n",
      "[14, 3500] Loss = 534.95710\n",
      "[14, 3750] Loss = 535.68322\n",
      "[14, 4000] Loss = 535.94799\n",
      "Loss iteration 14 = 535.88544\n",
      "[15, 250] Loss = 503.61553\n",
      "[15, 500] Loss = 495.18816\n",
      "[15, 750] Loss = 490.44055\n",
      "[15, 1000] Loss = 493.39611\n",
      "[15, 1250] Loss = 500.02186\n",
      "[15, 1500] Loss = 503.93375\n",
      "[15, 1750] Loss = 506.73272\n",
      "[15, 2000] Loss = 509.67064\n",
      "[15, 2250] Loss = 510.04432\n",
      "[15, 2500] Loss = 511.51653\n",
      "[15, 2750] Loss = 511.90165\n",
      "[15, 3000] Loss = 512.05911\n",
      "[15, 3250] Loss = 511.74153\n",
      "[15, 3500] Loss = 511.81671\n",
      "[15, 3750] Loss = 511.66248\n",
      "[15, 4000] Loss = 511.61208\n",
      "Loss iteration 15 = 511.77390\n",
      "[16, 250] Loss = 476.09473\n",
      "[16, 500] Loss = 464.56789\n",
      "[16, 750] Loss = 468.70565\n",
      "[16, 1000] Loss = 472.63962\n",
      "[16, 1250] Loss = 475.25009\n",
      "[16, 1500] Loss = 479.25493\n",
      "[16, 1750] Loss = 482.04538\n",
      "[16, 2000] Loss = 486.80039\n",
      "[16, 2250] Loss = 488.68818\n",
      "[16, 2500] Loss = 490.14164\n",
      "[16, 2750] Loss = 490.18180\n",
      "[16, 3000] Loss = 490.55136\n",
      "[16, 3250] Loss = 490.82176\n",
      "[16, 3500] Loss = 491.77850\n",
      "[16, 3750] Loss = 492.15522\n",
      "[16, 4000] Loss = 492.34514\n",
      "Loss iteration 16 = 492.30795\n",
      "[17, 250] Loss = 454.18094\n",
      "[17, 500] Loss = 461.61015\n",
      "[17, 750] Loss = 460.07935\n",
      "[17, 1000] Loss = 455.62622\n",
      "[17, 1250] Loss = 454.05933\n",
      "[17, 1500] Loss = 456.27850\n",
      "[17, 1750] Loss = 458.03293\n",
      "[17, 2000] Loss = 461.41193\n",
      "[17, 2250] Loss = 460.61031\n",
      "[17, 2500] Loss = 462.57218\n",
      "[17, 2750] Loss = 464.22920\n",
      "[17, 3000] Loss = 465.46363\n",
      "[17, 3250] Loss = 467.13058\n",
      "[17, 3500] Loss = 467.28234\n",
      "[17, 3750] Loss = 468.12384\n",
      "[17, 4000] Loss = 468.74611\n",
      "Loss iteration 17 = 468.80071\n",
      "[18, 250] Loss = 423.83251\n",
      "[18, 500] Loss = 433.22013\n",
      "[18, 750] Loss = 437.36374\n",
      "[18, 1000] Loss = 437.61763\n",
      "[18, 1250] Loss = 441.86286\n",
      "[18, 1500] Loss = 442.46004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1750] Loss = 442.06495\n",
      "[18, 2000] Loss = 444.31814\n",
      "[18, 2250] Loss = 445.95292\n",
      "[18, 2500] Loss = 446.40521\n",
      "[18, 2750] Loss = 447.21129\n",
      "[18, 3000] Loss = 448.72873\n",
      "[18, 3250] Loss = 448.00884\n",
      "[18, 3500] Loss = 448.25646\n",
      "[18, 3750] Loss = 448.83371\n",
      "[18, 4000] Loss = 449.05196\n",
      "Loss iteration 18 = 449.39509\n",
      "[19, 250] Loss = 403.95348\n",
      "[19, 500] Loss = 404.59403\n",
      "[19, 750] Loss = 409.69726\n",
      "[19, 1000] Loss = 413.88352\n",
      "[19, 1250] Loss = 415.96142\n",
      "[19, 1500] Loss = 420.82976\n",
      "[19, 1750] Loss = 422.38595\n",
      "[19, 2000] Loss = 423.73004\n",
      "[19, 2250] Loss = 425.91431\n",
      "[19, 2500] Loss = 426.91606\n",
      "[19, 2750] Loss = 428.16253\n",
      "[19, 3000] Loss = 430.34865\n",
      "[19, 3250] Loss = 431.15074\n",
      "[19, 3500] Loss = 432.04170\n",
      "[19, 3750] Loss = 433.87141\n",
      "[19, 4000] Loss = 434.97507\n",
      "Loss iteration 19 = 435.06808\n",
      "[20, 250] Loss = 391.19732\n",
      "[20, 500] Loss = 401.98063\n",
      "[20, 750] Loss = 406.21689\n",
      "[20, 1000] Loss = 404.93014\n",
      "[20, 1250] Loss = 408.17992\n",
      "[20, 1500] Loss = 408.92652\n",
      "[20, 1750] Loss = 412.33512\n",
      "[20, 2000] Loss = 413.07510\n",
      "[20, 2250] Loss = 412.72803\n",
      "[20, 2500] Loss = 414.74267\n",
      "[20, 2750] Loss = 415.99051\n",
      "[20, 3000] Loss = 416.92558\n",
      "[20, 3250] Loss = 419.24674\n",
      "[20, 3500] Loss = 418.69055\n",
      "[20, 3750] Loss = 419.35902\n",
      "[20, 4000] Loss = 420.14205\n",
      "Loss iteration 20 = 420.50320\n",
      "[21, 250] Loss = 388.98900\n",
      "[21, 500] Loss = 388.82116\n",
      "[21, 750] Loss = 387.02922\n",
      "[21, 1000] Loss = 389.77994\n",
      "[21, 1250] Loss = 392.39165\n",
      "[21, 1500] Loss = 392.77421\n",
      "[21, 1750] Loss = 395.16480\n",
      "[21, 2000] Loss = 396.51879\n",
      "[21, 2250] Loss = 398.91034\n",
      "[21, 2500] Loss = 399.50248\n",
      "[21, 2750] Loss = 400.88455\n",
      "[21, 3000] Loss = 402.95895\n",
      "[21, 3250] Loss = 403.66925\n",
      "[21, 3500] Loss = 404.45823\n",
      "[21, 3750] Loss = 404.38758\n",
      "[21, 4000] Loss = 405.30735\n",
      "Loss iteration 21 = 405.53303\n",
      "[22, 250] Loss = 371.63322\n",
      "[22, 500] Loss = 370.58639\n",
      "[22, 750] Loss = 372.15929\n",
      "[22, 1000] Loss = 372.99883\n",
      "[22, 1250] Loss = 375.83046\n",
      "[22, 1500] Loss = 378.64215\n",
      "[22, 1750] Loss = 381.21257\n",
      "[22, 2000] Loss = 382.35795\n",
      "[22, 2250] Loss = 385.71611\n",
      "[22, 2500] Loss = 387.72849\n",
      "[22, 2750] Loss = 388.71883\n",
      "[22, 3000] Loss = 388.67678\n",
      "[22, 3250] Loss = 390.15035\n",
      "[22, 3500] Loss = 391.00549\n",
      "[22, 3750] Loss = 391.60170\n",
      "[22, 4000] Loss = 393.23280\n",
      "Loss iteration 22 = 393.31386\n",
      "[23, 250] Loss = 370.79725\n",
      "[23, 500] Loss = 365.03178\n",
      "[23, 750] Loss = 365.75283\n",
      "[23, 1000] Loss = 365.89974\n",
      "[23, 1250] Loss = 365.83087\n",
      "[23, 1500] Loss = 367.43387\n",
      "[23, 1750] Loss = 370.97350\n",
      "[23, 2000] Loss = 372.72806\n",
      "[23, 2250] Loss = 374.23346\n",
      "[23, 2500] Loss = 374.01426\n",
      "[23, 2750] Loss = 375.52600\n",
      "[23, 3000] Loss = 375.84310\n",
      "[23, 3250] Loss = 377.57430\n",
      "[23, 3500] Loss = 378.08247\n",
      "[23, 3750] Loss = 378.93650\n",
      "[23, 4000] Loss = 379.09755\n",
      "Loss iteration 23 = 379.16326\n",
      "[24, 250] Loss = 350.21337\n",
      "[24, 500] Loss = 352.41610\n",
      "[24, 750] Loss = 353.08495\n",
      "[24, 1000] Loss = 355.04183\n",
      "[24, 1250] Loss = 357.72896\n",
      "[24, 1500] Loss = 359.44168\n",
      "[24, 1750] Loss = 360.20282\n",
      "[24, 2000] Loss = 360.39339\n",
      "[24, 2250] Loss = 360.97044\n",
      "[24, 2500] Loss = 361.77070\n",
      "[24, 2750] Loss = 362.62396\n",
      "[24, 3000] Loss = 363.38667\n",
      "[24, 3250] Loss = 365.05903\n",
      "[24, 3500] Loss = 366.35874\n",
      "[24, 3750] Loss = 367.27562\n",
      "[24, 4000] Loss = 368.78706\n",
      "Loss iteration 24 = 368.72826\n",
      "[25, 250] Loss = 346.63773\n",
      "[25, 500] Loss = 338.09201\n",
      "[25, 750] Loss = 340.38899\n",
      "[25, 1000] Loss = 343.00733\n",
      "[25, 1250] Loss = 345.86405\n",
      "[25, 1500] Loss = 347.50693\n",
      "[25, 1750] Loss = 349.25293\n",
      "[25, 2000] Loss = 351.80369\n",
      "[25, 2250] Loss = 353.53275\n",
      "[25, 2500] Loss = 356.01693\n",
      "[25, 2750] Loss = 357.12873\n",
      "[25, 3000] Loss = 358.56830\n",
      "[25, 3250] Loss = 358.74342\n",
      "[25, 3500] Loss = 359.91243\n",
      "[25, 3750] Loss = 359.56058\n",
      "[25, 4000] Loss = 360.29820\n",
      "Loss iteration 25 = 360.25473\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "(trained_net, adam_losses) = train(net, training_dataset, trainloader, losses, optimizer, criterion, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f94b3af50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_losses(adam_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store predictions in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "1250\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "predictions = get_predictions(net, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y,z,v) = testing_data_inp[0].shape\n",
    "predicted_brain = reconstruct_brain(predictions,[x,y,z,v])\n",
    "predicted_brain = replace_background(predicted_brain, testing_data_inp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 56, 46, 2)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_brain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate error on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test data: 8102.562012\n",
      "Mean absolute error on test data: 41.711079\n"
     ]
    }
   ],
   "source": [
    "mse = ((predicted_brain - testing_data_out[0]) ** 2).mean()\n",
    "print('MSE on test data: %f' % (mse))\n",
    "\n",
    "l1 = (np.absolute(predicted_brain - testing_data_out[0])).mean()\n",
    "print('Mean absolute error on test data: %f' % (l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f95718a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_pred(testing_data_inp[0], predicted_brain, testing_data_out[0], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123.1389"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_brain.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098.3759"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data_out[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the target and the predicted scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to = \"/home/ubuntu/project/Dataset/EXP_AWS_1/TrueModel\"\n",
    "\n",
    "predicted_scan = nib.Nifti1Image(predicted_brain, affine_mat)\n",
    "nib.save(predicted_scan, save_to + \"/Predicted_Subj1Scan2_AWS.nii.gz\" )\n",
    "\n",
    "target_scan = nib.Nifti1Image(testing_data_out[0][:,:,:,0], affine_mat)\n",
    "nib.save(target_scan, save_to + \"/Target_Subj1Scan2_AWS.nii.gz\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_net.state_dict(), '/home/ubuntu/project/MRIDataHarmonisation/Development/Model_Params_AWS/True_Model.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_net = Net()\n",
    "trained_net.cuda()\n",
    "trained_net.load_state_dict(torch.load('/home/ubuntu/project/MRIDataHarmonisation/Development/Model_Params_AWS/True_Model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31mPredicted_Subj7Scan2_AWS.nii.gz\u001b[0m  \u001b[01;31mTarget_Subj7Scan2_AWS.nii.gz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
